### SETTINGS ###
anchors = ['seitenansicht', 'draufsicht', 'deck', 'widerlager']
epochs = 100
margin = 120
learning_rate = 0.001
metric = l2-norm
loss_function = ContrastiveLoss
###

Epoch 1, Loss: 3143.7568359375
Epoch 2, Loss: 3123.890380859375
Epoch 3, Loss: 3233.977783203125
Epoch 4, Loss: 3269.696533203125
Epoch 5, Loss: 3265.138671875
Epoch 6, Loss: 3258.008056640625
Epoch 7, Loss: 3253.823486328125
Epoch 8, Loss: 3246.3876953125
Epoch 9, Loss: 3229.552978515625
Epoch 10, Loss: 3203.820556640625
Epoch 11, Loss: 3177.036376953125
Epoch 12, Loss: 3158.774169921875
Epoch 13, Loss: 3151.369384765625
Epoch 14, Loss: 3148.88330078125
Epoch 15, Loss: 3144.464599609375
Epoch 16, Loss: 3134.972900390625
Epoch 17, Loss: 3121.77880859375
Epoch 18, Loss: 3106.08154296875
Epoch 19, Loss: 3086.402587890625
Epoch 20, Loss: 3064.510009765625
Epoch 21, Loss: 3046.074951171875
Epoch 22, Loss: 3032.8564453125
Epoch 23, Loss: 3021.907958984375
Epoch 24, Loss: 3012.1318359375
Epoch 25, Loss: 3002.614013671875
Epoch 26, Loss: 2990.604248046875
Epoch 27, Loss: 2976.451171875
Epoch 28, Loss: 2961.02978515625
Epoch 29, Loss: 2944.353271484375
Epoch 30, Loss: 2927.48193359375
Epoch 31, Loss: 2910.490966796875
Epoch 32, Loss: 2892.684814453125
Epoch 33, Loss: 2874.031005859375
Epoch 34, Loss: 2854.3544921875
Epoch 35, Loss: 2833.583251953125
Epoch 36, Loss: 2812.01708984375
Epoch 37, Loss: 2789.546875
Epoch 38, Loss: 2765.8681640625
Epoch 39, Loss: 2740.7314453125
Epoch 40, Loss: 2713.7802734375
Epoch 41, Loss: 2684.928955078125
Epoch 42, Loss: 2654.758544921875
Epoch 43, Loss: 2624.619140625
Epoch 44, Loss: 2596.453857421875
Epoch 45, Loss: 2571.334228515625
Epoch 46, Loss: 2548.68359375
Epoch 47, Loss: 2527.58447265625
Epoch 48, Loss: 2506.82666015625
Epoch 49, Loss: 2485.417724609375
Epoch 50, Loss: 2463.084228515625
Epoch 51, Loss: 2950.533203125
Epoch 52, Loss: 2640.351318359375
Epoch 53, Loss: 2567.6875
Epoch 54, Loss: 2504.484619140625
Epoch 55, Loss: 2437.06396484375
Epoch 56, Loss: 2394.77197265625
Epoch 57, Loss: 2387.921630859375
Epoch 58, Loss: 2374.847412109375
Epoch 59, Loss: 2266.156982421875
Epoch 60, Loss: 2185.55712890625
Epoch 61, Loss: 2096.810791015625
Epoch 62, Loss: 1977.89697265625
Epoch 63, Loss: 1873.2545166015625
Epoch 64, Loss: 1747.228271484375
Epoch 65, Loss: 1616.657958984375
Epoch 66, Loss: 1513.1246337890625
Epoch 67, Loss: 1431.188720703125
Epoch 68, Loss: 1359.0830078125
Epoch 69, Loss: 1301.1700439453125
Epoch 70, Loss: 1241.048583984375
Epoch 71, Loss: 1192.5406494140625
Epoch 72, Loss: 1150.69677734375
Epoch 73, Loss: 1110.5526123046875
Epoch 74, Loss: 1071.6417236328125
Epoch 75, Loss: 1043.9722900390625
Epoch 76, Loss: 1012.9825439453125
Epoch 77, Loss: 995.5499877929688
Epoch 78, Loss: 962.6102294921875
Epoch 79, Loss: 962.3538208007812
Epoch 80, Loss: 922.5855712890625
Epoch 81, Loss: 956.8753662109375
Epoch 82, Loss: 992.5678100585938
Epoch 83, Loss: 1082.96044921875
Epoch 84, Loss: 1049.9097900390625
Epoch 85, Loss: 1021.2736206054688
Epoch 86, Loss: 883.71044921875
Epoch 87, Loss: 891.544677734375
Epoch 88, Loss: 931.4500732421875
Epoch 89, Loss: 827.6316528320312
Epoch 90, Loss: 828.9616088867188
Epoch 91, Loss: 852.3963623046875
Epoch 92, Loss: 795.4443969726562
Epoch 93, Loss: 821.8277587890625
Epoch 94, Loss: 801.2244873046875
Epoch 95, Loss: 840.0515747070312
Epoch 96, Loss: 848.7227172851562
Epoch 97, Loss: 858.908447265625
Epoch 98, Loss: 754.9133911132812
Epoch 99, Loss: 784.7471313476562
Epoch 100, Loss: 751.6922607421875
	 -> training-time: 61.55 min.
