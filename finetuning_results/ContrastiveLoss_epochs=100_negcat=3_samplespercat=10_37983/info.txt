### SETTINGS ###
anchor_drawing = house
negative_drawings = ['airplane', 'face', 'bathtub']
number of samples per drawing category = 10
epochs = 100
margin = 120
learning_rate = 0.0005
metric = l2-norm
loss_function = ContrastiveLoss
###

before:
pos = 52.31465530395508, 46.507450103759766 	(avg: 49.2095947265625)
neg1 = 70.01513671875, 66.1571044921875 	(avg: 62.98496627807617)
neg2 = 61.81850814819336, 66.1571044921875 	(avg: 61.98998260498047)

Epoch 1, Loss: 3078.15478515625
Epoch 2, Loss: 3053.101318359375
Epoch 3, Loss: 2785.744384765625
Epoch 4, Loss: 2697.97021484375
Epoch 5, Loss: 2651.2490234375
Epoch 6, Loss: 2590.58740234375
Epoch 7, Loss: 2536.78759765625
Epoch 8, Loss: 2503.65771484375
Epoch 9, Loss: 2488.80029296875
Epoch 10, Loss: 2480.799560546875
Epoch 11, Loss: 2468.85693359375
Epoch 12, Loss: 2451.681640625
Epoch 13, Loss: 2432.710205078125
Epoch 14, Loss: 2416.332763671875
Epoch 15, Loss: 2406.679931640625
Epoch 16, Loss: 2402.4736328125
Epoch 17, Loss: 2397.3564453125
Epoch 18, Loss: 2388.4404296875
Epoch 19, Loss: 2377.70068359375
Epoch 20, Loss: 2367.2724609375
Epoch 21, Loss: 2357.245361328125
Epoch 22, Loss: 2346.886474609375
Epoch 23, Loss: 2335.93798828125
Epoch 24, Loss: 2324.699462890625
Epoch 25, Loss: 2313.6865234375
Epoch 26, Loss: 2303.31298828125
Epoch 27, Loss: 2293.66943359375
Epoch 28, Loss: 2284.54736328125
Epoch 29, Loss: 2275.803955078125
Epoch 30, Loss: 2267.460693359375
Epoch 31, Loss: 2259.443603515625
Epoch 32, Loss: 2251.51416015625
Epoch 33, Loss: 2243.456787109375
Epoch 34, Loss: 2235.39501953125
Epoch 35, Loss: 2227.8330078125
Epoch 36, Loss: 2221.19677734375
Epoch 37, Loss: 2215.306884765625
Epoch 38, Loss: 2209.470458984375
Epoch 39, Loss: 2203.045654296875
Epoch 40, Loss: 2195.908447265625
Epoch 41, Loss: 2188.47119140625
Epoch 42, Loss: 2181.355224609375
Epoch 43, Loss: 2174.963623046875
Epoch 44, Loss: 2169.269287109375
Epoch 45, Loss: 2163.948974609375
Epoch 46, Loss: 2158.6748046875
Epoch 47, Loss: 2153.3447265625
Epoch 48, Loss: 2148.049072265625
Epoch 49, Loss: 2142.88818359375
Epoch 50, Loss: 2137.868896484375
Epoch 51, Loss: 2132.933349609375
Epoch 52, Loss: 2128.047119140625
Epoch 53, Loss: 2123.224609375
Epoch 54, Loss: 2118.486572265625
Epoch 55, Loss: 2113.85546875
Epoch 56, Loss: 2109.355224609375
Epoch 57, Loss: 2105.0146484375
Epoch 58, Loss: 2100.82958984375
Epoch 59, Loss: 2096.744140625
Epoch 60, Loss: 2092.68994140625
Epoch 61, Loss: 2088.650390625
Epoch 62, Loss: 2084.6591796875
Epoch 63, Loss: 2080.741455078125
Epoch 64, Loss: 2076.87646484375
Epoch 65, Loss: 2073.0244140625
Epoch 66, Loss: 2069.183349609375
Epoch 67, Loss: 2065.408203125
Epoch 68, Loss: 2061.72900390625
Epoch 69, Loss: 2058.127197265625
Epoch 70, Loss: 2054.553955078125
Epoch 71, Loss: 2050.994140625
Epoch 72, Loss: 2047.4630126953125
Epoch 73, Loss: 2043.9732666015625
Epoch 74, Loss: 2040.501220703125
Epoch 75, Loss: 2037.0093994140625
Epoch 76, Loss: 2033.50390625
Epoch 77, Loss: 2030.0096435546875
Epoch 78, Loss: 2026.5472412109375
Epoch 79, Loss: 2023.102294921875
Epoch 80, Loss: 2019.65283203125
Epoch 81, Loss: 2016.2052001953125
Epoch 82, Loss: 2012.7833251953125
Epoch 83, Loss: 2009.3927001953125
Epoch 84, Loss: 2006.0206298828125
Epoch 85, Loss: 2002.65966796875
Epoch 86, Loss: 1999.318603515625
Epoch 87, Loss: 1996.014404296875
Epoch 88, Loss: 1992.7420654296875
Epoch 89, Loss: 1989.495361328125
Epoch 90, Loss: 1986.271240234375
Epoch 91, Loss: 1983.0718994140625
Epoch 92, Loss: 1979.8946533203125
Epoch 93, Loss: 1976.7418212890625
Epoch 94, Loss: 1973.60888671875
Epoch 95, Loss: 1970.4940185546875
Epoch 96, Loss: 1967.393310546875
Epoch 97, Loss: 1964.3109130859375
Epoch 98, Loss: 1961.2493896484375
Epoch 99, Loss: 1958.207763671875
Epoch 100, Loss: 1955.1796875
	 -> training-time: 21.53 min.

after:
pos = 67.01948547363281, 51.06825256347656 	(avg: 61.474464416503906)
neg1 = 92.53115844726562, 84.64917755126953 	(avg: 84.69866180419922)
neg2 = 83.92658996582031, 84.64917755126953 	(avg: 82.68132019042969)

